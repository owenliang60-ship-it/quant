#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¸å®‰åˆçº¦ PMARP ä¿¡å·æ‰«æå™¨
æ¯å¤©æ—©ä¸Š8ç‚¹æ‰«æUSDTåˆçº¦ï¼Œæ£€æµ‹PMARPæ—¥çº¿ä¸Šç©¿98%çš„ä¿¡å·

Author: Claude Code
Date: 2026-01-02
"""

import os
import time
import requests
import pandas as pd
from datetime import datetime, timedelta, timezone
from pathlib import Path

# ============================================================
# è·¯å¾„é…ç½® (ç›¸å¯¹è·¯å¾„ï¼Œæ”¯æŒæ–‡ä»¶å¤¹ç§»åŠ¨)
# ============================================================
SCRIPT_DIR = Path(__file__).resolve().parent  # scanners/
PROJECT_ROOT = SCRIPT_DIR.parent              # C:\Quant\ (æˆ–ä»»æ„ä½ç½®)

# ============================================================
# é…ç½®
# ============================================================
CONFIG = {
    # å¸å®‰åˆçº¦APIï¼ˆæ— éœ€ä»£ç†ï¼‰
    "base_url": "https://fapi.binance.com",

    # PMARPå‚æ•°
    "ema_period": 20,
    "lookback": 150,
    "threshold": 98,

    # è¿‡æ»¤æ¡ä»¶
    "top_n": 50,  # åªæ‰«ææˆäº¤é‡å‰50

    # æ•°æ®ç¼“å­˜ (ç›¸å¯¹è·¯å¾„)
    "cache_dir": str(PROJECT_ROOT / "cache" / "binance_daily_cache"),
    "history_days": 365,

    # Telegramé…ç½® (ä»ç¯å¢ƒå˜é‡è¯»å–)
    "telegram_bot_token": os.environ.get("TELEGRAM_BOT_TOKEN", ""),
    "telegram_chat_id": os.environ.get("TELEGRAM_CHAT_ID", ""),
    "proxy": "",  # ä»…Telegraméœ€è¦
}

# ============================================================
# å¸å®‰APIå°è£…ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
# ============================================================
def api_request_with_retry(url: str, params: dict = None, max_retries: int = 3, timeout: int = 30) -> dict:
    """
    å¸¦é‡è¯•æœºåˆ¶çš„APIè¯·æ±‚

    Args:
        url: è¯·æ±‚URL
        params: è¯·æ±‚å‚æ•°
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

    Returns:
        å“åº”æ•°æ®ï¼Œå¤±è´¥è¿”å›None
    """
    for attempt in range(1, max_retries + 1):
        try:
            resp = requests.get(url, params=params, timeout=timeout)
            resp.raise_for_status()
            return resp.json()
        except requests.exceptions.Timeout:
            print(f"[API] ç¬¬{attempt}æ¬¡è¯·æ±‚è¶…æ—¶...")
        except requests.exceptions.RequestException as e:
            print(f"[API] ç¬¬{attempt}æ¬¡è¯·æ±‚å¤±è´¥: {e}")

        if attempt < max_retries:
            wait_time = attempt * 2
            print(f"[API] {wait_time}ç§’åé‡è¯•...")
            time.sleep(wait_time)

    return None


def get_all_usdt_futures() -> list:
    """è·å–æ‰€æœ‰USDTæ°¸ç»­åˆçº¦äº¤æ˜“å¯¹"""
    url = f"{CONFIG['base_url']}/fapi/v1/exchangeInfo"

    data = api_request_with_retry(url)

    if data is None:
        print(f"[é”™è¯¯] è·å–äº¤æ˜“å¯¹åˆ—è¡¨å¤±è´¥ï¼ˆå·²é‡è¯•3æ¬¡ï¼‰")
        return None  # è¿”å›Noneè¡¨ç¤ºAPIå¤±è´¥

    symbols = []
    for s in data.get("symbols", []):
        if (s.get("quoteAsset") == "USDT" and
            s.get("contractType") == "PERPETUAL" and
            s.get("status") == "TRADING"):
            symbols.append(s["symbol"])

    print(f"[API] è·å–åˆ° {len(symbols)} ä¸ªUSDTæ°¸ç»­åˆçº¦")
    return symbols


def get_24h_volume() -> dict:
    """è·å–æ‰€æœ‰åˆçº¦24hæˆäº¤é‡ï¼ˆUSDTè®¡ä»·ï¼‰"""
    url = f"{CONFIG['base_url']}/fapi/v1/ticker/24hr"

    data = api_request_with_retry(url)

    if data is None:
        print(f"[é”™è¯¯] è·å–24hæˆäº¤é‡å¤±è´¥ï¼ˆå·²é‡è¯•3æ¬¡ï¼‰")
        return None  # è¿”å›Noneè¡¨ç¤ºAPIå¤±è´¥

    volumes = {}
    for item in data:
        symbol = item.get("symbol", "")
        quote_volume = float(item.get("quoteVolume", 0))
        volumes[symbol] = quote_volume

    return volumes


def get_yesterday_daily_volume(symbol: str) -> float:
    """è·å–æŸä¸ªå¸ç§æ˜¨æ—¥æ—¥çº¿æˆäº¤é‡ï¼ˆUSDTè®¡ä»·ï¼‰"""
    cache_path = get_cache_path(symbol)
    if not cache_path.exists():
        return 0

    try:
        df = pd.read_csv(cache_path, parse_dates=["timestamp"])
        if len(df) < 2:
            return 0
        df = df.sort_values("timestamp")
        # è¿”å›æœ€æ–°ä¸€æ ¹æ—¥çº¿çš„USDTæˆäº¤é‡
        if "quote_volume" in df.columns:
            return float(df["quote_volume"].iloc[-1])
        return 0  # æ—§ç¼“å­˜æ²¡æœ‰quote_volumeï¼Œè¿”å›0
    except:
        return 0


def get_top_n_symbols(n: int = 50) -> list:
    """
    è·å–æˆäº¤é‡å‰Nçš„USDTåˆçº¦

    ä¼˜å…ˆä½¿ç”¨æ—¥çº¿æˆäº¤é‡ï¼ˆæ›´å‡†ç¡®ï¼‰ï¼Œå¦‚æœç¼“å­˜ä¸è¶³åˆ™ç”¨24hæˆäº¤é‡
    """
    all_symbols = get_all_usdt_futures()

    if all_symbols is None:
        return None  # APIå¤±è´¥

    # æ–¹æ¡ˆ1: å…ˆå°è¯•ç”¨ç¼“å­˜çš„æ—¥çº¿æˆäº¤é‡ç­›é€‰
    # è¿™æ ·å¯ä»¥é¿å…24hæ»šåŠ¨çª—å£å¯¼è‡´æ¼æ‰æ”¾é‡å¸ç§
    cache_dir = Path(CONFIG["cache_dir"])
    symbol_volumes = []

    for s in all_symbols:
        cache_path = cache_dir / f"{s}.csv"
        if cache_path.exists():
            vol = get_yesterday_daily_volume(s)
            if vol > 0:
                symbol_volumes.append((s, vol))

    # å¦‚æœç¼“å­˜æ•°æ®è¶³å¤Ÿï¼ˆè‡³å°‘æœ‰100ä¸ªå¸ç§æœ‰ç¼“å­˜ï¼‰ï¼Œç”¨æ—¥çº¿æˆäº¤é‡ç­›é€‰
    if len(symbol_volumes) >= 100:
        symbol_volumes.sort(key=lambda x: x[1], reverse=True)
        top_symbols = [s[0] for s in symbol_volumes[:n]]
        print(f"[ç­›é€‰] ä½¿ç”¨æ—¥çº¿æˆäº¤é‡ç­›é€‰å‰{n}: {top_symbols[:5]}...")
        return top_symbols

    # æ–¹æ¡ˆ2: ç¼“å­˜ä¸è¶³ï¼Œç”¨24hæˆäº¤é‡ç­›é€‰ï¼ˆé¦–æ¬¡è¿è¡Œæ—¶ï¼‰
    print("[ç­›é€‰] ç¼“å­˜ä¸è¶³ï¼Œä½¿ç”¨24hæˆäº¤é‡ç­›é€‰...")
    volumes = get_24h_volume()

    if volumes is None:
        return None  # APIå¤±è´¥

    symbol_volumes = []
    for s in all_symbols:
        if s in volumes:
            symbol_volumes.append((s, volumes[s]))

    symbol_volumes.sort(key=lambda x: x[1], reverse=True)
    top_symbols = [s[0] for s in symbol_volumes[:n]]
    print(f"[ç­›é€‰] æˆäº¤é‡å‰{n}çš„åˆçº¦: {top_symbols[:5]}...")

    return top_symbols


def fetch_klines(symbol: str, interval: str = "1d", limit: int = 500) -> list:
    """è·å–Kçº¿æ•°æ®ï¼ˆå¸¦é‡è¯•ï¼‰"""
    url = f"{CONFIG['base_url']}/fapi/v1/klines"
    params = {
        "symbol": symbol,
        "interval": interval,
        "limit": limit
    }

    data = api_request_with_retry(url, params=params, max_retries=2, timeout=30)

    if data is None:
        print(f"[é”™è¯¯] è·å–{symbol}Kçº¿å¤±è´¥ï¼ˆå·²é‡è¯•ï¼‰")
        return []

    return data


def fetch_klines_by_time(symbol: str, start_time: int, end_time: int = None,
                         interval: str = "1d") -> list:
    """æŒ‰æ—¶é—´èŒƒå›´è·å–Kçº¿æ•°æ®ï¼ˆå¸¦é‡è¯•ï¼‰"""
    url = f"{CONFIG['base_url']}/fapi/v1/klines"
    params = {
        "symbol": symbol,
        "interval": interval,
        "startTime": start_time,
        "limit": 1500
    }
    if end_time:
        params["endTime"] = end_time

    data = api_request_with_retry(url, params=params, max_retries=2, timeout=30)

    if data is None:
        print(f"[é”™è¯¯] è·å–{symbol}Kçº¿å¤±è´¥ï¼ˆå·²é‡è¯•ï¼‰")
        return []

    return data


# ============================================================
# æ•°æ®ç¼“å­˜æ¨¡å—
# ============================================================
def ensure_cache_dir():
    """ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨"""
    cache_dir = Path(CONFIG["cache_dir"])
    cache_dir.mkdir(parents=True, exist_ok=True)
    return cache_dir


def get_cache_path(symbol: str) -> Path:
    """è·å–æŸä¸ªå¸ç§çš„ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
    cache_dir = ensure_cache_dir()
    return cache_dir / f"{symbol}.csv"


def load_cached_data(symbol: str) -> pd.DataFrame:
    """åŠ è½½ç¼“å­˜çš„å†å²æ•°æ®"""
    cache_path = get_cache_path(symbol)

    if not cache_path.exists():
        return pd.DataFrame()

    try:
        df = pd.read_csv(cache_path, parse_dates=["timestamp"])
        return df
    except Exception as e:
        print(f"[è­¦å‘Š] è¯»å–{symbol}ç¼“å­˜å¤±è´¥: {e}")
        return pd.DataFrame()


def save_cached_data(symbol: str, df: pd.DataFrame):
    """ä¿å­˜æ•°æ®åˆ°ç¼“å­˜"""
    cache_path = get_cache_path(symbol)
    df.to_csv(cache_path, index=False)


def klines_to_dataframe(klines: list) -> pd.DataFrame:
    """
    å°†Kçº¿æ•°æ®è½¬æ¢ä¸ºDataFrame

    å¸å®‰Kçº¿æ•°æ®æ ¼å¼:
    k[5] = volume (åŸºç¡€è´§å¸æˆäº¤é‡ï¼Œå¦‚BTCä¸ªæ•°)
    k[7] = quoteVolume (æŠ¥ä»·è´§å¸æˆäº¤é‡ï¼Œå¦‚USDTé‡‘é¢)
    """
    if not klines:
        return pd.DataFrame()

    records = []
    for k in klines:
        records.append({
            "timestamp": pd.to_datetime(k[0], unit="ms"),
            "open": float(k[1]),
            "high": float(k[2]),
            "low": float(k[3]),
            "close": float(k[4]),
            "volume": float(k[5]),
            "quote_volume": float(k[7]),  # USDTæˆäº¤é‡
        })

    df = pd.DataFrame(records)
    return df


def filter_closed_klines(df: pd.DataFrame) -> pd.DataFrame:
    """
    è¿‡æ»¤æ‰æ­£åœ¨è¿›è¡Œä¸­çš„æ—¥çº¿ï¼Œåªä¿ç•™å·²æ”¶ç›˜çš„
    å¸å®‰æ—¥çº¿åœ¨ UTC 00:00 æ”¶ç›˜ï¼Œæ‰€ä»¥åªä¿ç•™æ—¥æœŸ < ä»Šå¤©UTCæ—¥æœŸçš„æ•°æ®
    """
    if df.empty:
        return df
    today_utc = pd.Timestamp(datetime.now(timezone.utc).replace(tzinfo=None)).normalize()
    return df[df["timestamp"] < today_utc]


def update_daily_cache(symbol: str) -> pd.DataFrame:
    """
    æ›´æ–°æŸä¸ªå¸ç§çš„æ—¥çº¿æ•°æ®ç¼“å­˜
    - å¦‚æœæ— ç¼“å­˜ï¼Œä¸‹è½½å®Œæ•´365å¤©æ•°æ®
    - å¦‚æœæœ‰ç¼“å­˜ï¼Œå¼ºåˆ¶åˆ·æ–°æœ€è¿‘3å¤©æ•°æ®

    Bugä¿®å¤è¯´æ˜ (2026-01-14):
    ä¹‹å‰çš„é€»è¾‘åªæ£€æŸ¥æ—¥æœŸï¼Œä¸æ£€æŸ¥æ•°æ®æ˜¯å¦å®Œæ•´ã€‚
    å¦‚æœå‰ä¸€å¤©ä¿å­˜äº†æ­£åœ¨è¿›è¡Œä¸­çš„æ—¥çº¿ï¼ˆä¸å®Œæ•´æ•°æ®ï¼‰ï¼Œ
    ä»Šå¤©å°±ä¸ä¼šæ›´æ–°ï¼Œå¯¼è‡´PMARPè®¡ç®—é”™è¯¯ï¼Œæ£€æµ‹ä¸åˆ°ä¿¡å·ã€‚
    ç°åœ¨æ”¹ä¸ºæ¯å¤©å¼ºåˆ¶åˆ·æ–°æœ€è¿‘3å¤©çš„æ•°æ®ï¼Œç¡®ä¿ä½¿ç”¨å·²æ”¶ç›˜çš„å®Œæ•´æ•°æ®ã€‚

    Returns:
        æ›´æ–°åçš„å®Œæ•´DataFrame
    """
    cache_df = load_cached_data(symbol)

    if cache_df.empty:
        # æ— ç¼“å­˜ï¼Œä¸‹è½½å®Œæ•´å†å²
        print(f"[ç¼“å­˜] {symbol}: é¦–æ¬¡ä¸‹è½½ï¼Œè·å–{CONFIG['history_days']}å¤©æ•°æ®...")

        start_time = int((datetime.now(timezone.utc) - timedelta(days=CONFIG['history_days'])).timestamp() * 1000)
        klines = fetch_klines_by_time(symbol, start_time)

        if not klines:
            return pd.DataFrame()

        df = klines_to_dataframe(klines)
        df = filter_closed_klines(df)  # è¿‡æ»¤æ‰è¿›è¡Œä¸­çš„æ—¥çº¿
        save_cached_data(symbol, df)
        print(f"[ç¼“å­˜] {symbol}: å·²ä¿å­˜ {len(df)} æ¡è®°å½•")
        return df

    else:
        # æœ‰ç¼“å­˜ï¼Œå¼ºåˆ¶åˆ·æ–°æœ€è¿‘3å¤©æ•°æ®
        today = pd.Timestamp(datetime.now(timezone.utc).replace(tzinfo=None)).normalize()
        refresh_start = today - timedelta(days=3)

        # æ£€æŸ¥æ—§ç¼“å­˜æ˜¯å¦æœ‰ quote_volume åˆ—
        if "quote_volume" not in cache_df.columns:
            print(f"[ç¼“å­˜] {symbol}: æ—§ç¼“å­˜ç¼ºå°‘ quote_volumeï¼Œé‡æ–°ä¸‹è½½å®Œæ•´æ•°æ®...")
            cache_path = get_cache_path(symbol)
            if cache_path.exists():
                cache_path.unlink()
            return update_daily_cache(symbol)  # é€’å½’è°ƒç”¨ï¼Œèµ°é¦–æ¬¡ä¸‹è½½é€»è¾‘

        # åˆ é™¤ç¼“å­˜ä¸­æœ€è¿‘3å¤©çš„æ•°æ®
        cache_df = cache_df[cache_df["timestamp"] < refresh_start]

        # ä¸‹è½½æœ€è¿‘çš„æ•°æ®
        start_time = int(refresh_start.timestamp() * 1000)
        klines = fetch_klines_by_time(symbol, start_time)

        if klines:
            new_df = klines_to_dataframe(klines)
            new_df = filter_closed_klines(new_df)  # è¿‡æ»¤æ‰è¿›è¡Œä¸­çš„æ—¥çº¿

            # åˆå¹¶å¹¶å»é‡
            df = pd.concat([cache_df, new_df], ignore_index=True)
            df = df.drop_duplicates(subset=["timestamp"], keep="last")
            df = df.sort_values("timestamp").reset_index(drop=True)

            # åªä¿ç•™æœ€è¿‘365å¤©
            cutoff = pd.Timestamp(datetime.now(timezone.utc).replace(tzinfo=None)) - timedelta(days=CONFIG['history_days'])
            df = df[df["timestamp"] >= cutoff]

            save_cached_data(symbol, df)
            return df

        return cache_df


# ============================================================
# PMARPè®¡ç®—æ¨¡å—
# ============================================================
def calculate_pmarp(prices: pd.Series, ema_period: int = 20, lookback: int = 150) -> pd.Series:
    """
    è®¡ç®—PMARP (Price Moving Average Ratio Percentile)

    PMAR = Price / EMA(Price, period)
    PMARP = åœ¨è¿‡å»lookbackæ ¹Kçº¿ä¸­ï¼Œæœ‰å¤šå°‘æ¯”ä¾‹çš„PMARå¤§äºå½“å‰PMAR

    PMARPä½ (å¦‚<2%) = ä»·æ ¼å¤„äºå†å²æä½ä½ï¼ˆè¶…å–ï¼‰
    PMARPé«˜ (å¦‚>98%) = ä»·æ ¼å¤„äºå†å²æé«˜ä½ï¼ˆè¶…ä¹°/è¿½æ¶¨ï¼‰

    Args:
        prices: æ”¶ç›˜ä»·åºåˆ—
        ema_period: EMAå‘¨æœŸ
        lookback: å›çœ‹å‘¨æœŸ

    Returns:
        PMARPåºåˆ— (0-100)
    """
    # è®¡ç®—EMA
    ema = prices.ewm(span=ema_period, adjust=False).mean()

    # è®¡ç®—PMAR
    pmar = prices / ema

    # è®¡ç®—PMARP
    pmarp = pd.Series(index=pmar.index, dtype=float)

    for i in range(lookback, len(pmar)):
        current = pmar.iloc[i]
        historical = pmar.iloc[i-lookback:i]

        # è®¡ç®—æœ‰å¤šå°‘å†å²PMARå°äºç­‰äºå½“å‰PMAR
        count_le = (historical <= current).sum()

        # PMARP = å°äºç­‰äºå½“å‰å€¼çš„æ¯”ä¾‹ * 100
        # PMARPé«˜ = å½“å‰å¤„äºå†å²é«˜ä½ï¼ˆå¼ºåŠ¿ï¼‰
        pmarp.iloc[i] = count_le / lookback * 100

    return pmarp


def check_crossover_98(pmarp: pd.Series, threshold: float = 98) -> bool:
    """
    æ£€æµ‹æ˜¯å¦å‘ç”Ÿä¸Šç©¿98%

    æ¡ä»¶: å‰ä¸€å¤© PMARP < 98% AND å½“å¤© PMARP >= 98%

    Args:
        pmarp: PMARPåºåˆ—
        threshold: é˜ˆå€¼ï¼Œé»˜è®¤98

    Returns:
        æ˜¯å¦è§¦å‘ä¿¡å·
    """
    if len(pmarp) < 2:
        return False

    # å»æ‰NaNå€¼
    valid_pmarp = pmarp.dropna()
    if len(valid_pmarp) < 2:
        return False

    prev_value = valid_pmarp.iloc[-2]
    curr_value = valid_pmarp.iloc[-1]

    # ä¸Šç©¿æ¡ä»¶: å‰ä¸€å¤© < threshold AND å½“å¤© >= threshold
    return prev_value < threshold and curr_value >= threshold


# ============================================================
# Telegramæ¨é€æ¨¡å—
# ============================================================
def get_proxies():
    """è·å–ä»£ç†é…ç½®"""
    proxy = CONFIG.get("proxy", "")
    if proxy:
        return {"http": proxy, "https": proxy}
    return None


def send_telegram_alert(message: str, max_retries: int = 3) -> bool:
    """å‘é€Telegramæ¶ˆæ¯ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
    token = CONFIG["telegram_bot_token"]
    chat_id = CONFIG["telegram_chat_id"]

    if not token or not chat_id:
        print(f"[è­¦å‘Š] Telegramæœªé…ç½®ï¼Œè·³è¿‡å‘é€")
        print("[æ¶ˆæ¯å†…å®¹]\n" + message)
        return False

    url = f"https://api.telegram.org/bot{token}/sendMessage"
    payload = {
        "chat_id": chat_id,
        "text": message,
        "parse_mode": "Markdown"
    }

    proxies = get_proxies()

    for attempt in range(1, max_retries + 1):
        try:
            response = requests.post(url, json=payload, timeout=15, proxies=proxies)
            response.raise_for_status()
            print(f"[Telegram] æ¶ˆæ¯å·²å‘é€")
            return True
        except Exception as e:
            print(f"[Telegram] ç¬¬{attempt}æ¬¡å‘é€å¤±è´¥: {e}")
            if attempt < max_retries:
                wait_time = attempt * 2
                print(f"[Telegram] {wait_time}ç§’åé‡è¯•...")
                time.sleep(wait_time)

    print(f"[Telegram] å‘é€å¤±è´¥ï¼Œå·²é‡è¯•{max_retries}æ¬¡")
    return False

    url = f"https://api.telegram.org/bot{token}/sendMessage"
    payload = {
        "chat_id": chat_id,
        "text": message,
        "parse_mode": "Markdown"
    }

    try:
        proxies = get_proxies()
        response = requests.post(url, json=payload, timeout=15, proxies=proxies)
        response.raise_for_status()
        print(f"[Telegram] æ¶ˆæ¯å·²å‘é€")
        return True
    except Exception as e:
        print(f"[Telegram] å‘é€å¤±è´¥: {e}")
        return False


def format_signal_message(signals: list) -> str:
    """
    æ ¼å¼åŒ–ä¿¡å·æ¶ˆæ¯

    Args:
        signals: ä¿¡å·åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºdict

    Returns:
        æ ¼å¼åŒ–çš„æ¶ˆæ¯å­—ç¬¦ä¸²
    """
    now = datetime.now().strftime("%Y-%m-%d %H:%M")

    msg = f"*PMARP ä¿¡å·æé†’*\n"
    msg += f"æ—¶é—´: {now}\n\n"
    msg += f"è§¦å‘å¸ç§ ({len(signals)}ä¸ª):\n"

    for i, s in enumerate(signals, 1):
        symbol = s["symbol"]
        price = s["price"]
        pmarp = s["pmarp"]
        prev_pmarp = s["prev_pmarp"]
        is_first = s.get("is_first_scan", False)

        if is_first:
            msg += f"{i}. *{symbol}* ğŸ†• | ${price:,.4f} | PMARP: {pmarp:.1f}% (æ–°å¸é¦–æ¬¡æ‰«æ)\n"
        else:
            msg += f"{i}. *{symbol}* | ${price:,.4f} | PMARP: {prev_pmarp:.1f}% â†’ {pmarp:.1f}%\n"

    msg += f"\næ‰«æèŒƒå›´: 24hæˆäº¤é‡å‰{CONFIG['top_n']}"

    return msg


# ============================================================
# æ•°æ®é¢„æ£€æŸ¥æ¨¡å—
# ============================================================
def get_expected_latest_date():
    """
    è·å–é¢„æœŸçš„æœ€æ–°æ—¥çº¿æ—¥æœŸ
    å¸å®‰æ—¥çº¿åœ¨ UTC 00:00 æ”¶ç›˜ï¼Œå³åŒ—äº¬æ—¶é—´ 08:00
    """
    now_utc = datetime.now(timezone.utc)
    expected = (now_utc - timedelta(days=1)).replace(hour=0, minute=0, second=0, microsecond=0)
    return pd.Timestamp(expected.replace(tzinfo=None))


def ensure_data_updated() -> bool:
    """
    ç¡®ä¿æ•°æ®å·²æ›´æ–°åˆ°æœ€æ–°ï¼Œä¸”åŒ…å« quote_volume å­—æ®µ
    å¦‚æœæ•°æ®è¿‡æ—§æˆ–æ ¼å¼ä¸å¯¹ï¼Œå°è¯•æ›´æ–°éªŒè¯

    Returns:
        True å¦‚æœæ•°æ®å¯ç”¨ï¼ŒFalse å¦‚æœæ›´æ–°å¤±è´¥
    """
    expected_date = get_expected_latest_date()
    cache_df = load_cached_data("BTCUSDT")

    # æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨
    if cache_df.empty:
        print(f"[é¢„æ£€æŸ¥] BTCUSDT æ— ç¼“å­˜ï¼Œéœ€è¦ä¸‹è½½...")
    else:
        last_date = cache_df["timestamp"].max()
        has_quote_volume = "quote_volume" in cache_df.columns

        if last_date >= expected_date and has_quote_volume:
            print(f"[é¢„æ£€æŸ¥] æ•°æ®å·²æ˜¯æœ€æ–° (ç¼“å­˜: {last_date.strftime('%Y-%m-%d')})")
            print(f"[é¢„æ£€æŸ¥] æ•°æ®æ ¼å¼æ­£ç¡®ï¼ŒåŒ…å« quote_volume å­—æ®µ")
            return True

        if not has_quote_volume:
            print(f"[é¢„æ£€æŸ¥] ç¼“å­˜ç¼ºå°‘ quote_volume å­—æ®µï¼Œéœ€è¦é‡æ–°ä¸‹è½½")
        else:
            print(f"[é¢„æ£€æŸ¥] æ•°æ®éœ€è¦æ›´æ–° (ç¼“å­˜: {last_date.strftime('%Y-%m-%d')}, é¢„æœŸ: {expected_date.strftime('%Y-%m-%d')})")

    # å°è¯•æ›´æ–° BTCUSDT éªŒè¯ API
    print(f"[é¢„æ£€æŸ¥] å°è¯•æ›´æ–° BTCUSDT éªŒè¯API...")

    # å¼ºåˆ¶æ¸…ç©ºç¼“å­˜é‡æ–°ä¸‹è½½
    cache_path = get_cache_path("BTCUSDT")
    if cache_path.exists():
        cache_path.unlink()

    df = update_daily_cache("BTCUSDT")

    if df.empty:
        print(f"[é¢„æ£€æŸ¥] APIæ›´æ–°å¤±è´¥ï¼")
        return False

    if "quote_volume" not in df.columns:
        print(f"[é¢„æ£€æŸ¥] ä¸‹è½½çš„æ•°æ®ç¼ºå°‘ quote_volume å­—æ®µï¼")
        return False

    new_last_date = df["timestamp"].max()
    print(f"[é¢„æ£€æŸ¥] APIéªŒè¯æˆåŠŸï¼Œæ•°æ®å·²æ›´æ–°åˆ° {new_last_date.strftime('%Y-%m-%d')}")
    return True


def validate_cache_integrity(symbols: list) -> list:
    """
    æ ¡éªŒç¼“å­˜æ•°æ®å®Œæ•´æ€§ï¼Œè¿”å›éœ€è¦é‡æ–°ä¸‹è½½çš„å¸ç§åˆ—è¡¨

    æ£€æŸ¥é¡¹:
    1. quote_volume åˆ—æ˜¯å¦å­˜åœ¨
    2. æœ€è¿‘3å¤©çš„ quote_volume æ˜¯å¦æœ‰æ•ˆï¼ˆéç©ºã€é0ï¼‰
    """
    invalid_symbols = []
    cache_dir = Path(CONFIG["cache_dir"])

    for symbol in symbols:
        cache_path = cache_dir / f"{symbol}.csv"
        if not cache_path.exists():
            continue

        try:
            df = pd.read_csv(cache_path, parse_dates=["timestamp"])

            # æ£€æŸ¥1: quote_volume åˆ—æ˜¯å¦å­˜åœ¨
            if "quote_volume" not in df.columns:
                print(f"[æ ¡éªŒ] {symbol}: ç¼ºå°‘ quote_volume åˆ—")
                invalid_symbols.append(symbol)
                continue

            # æ£€æŸ¥2: æœ€è¿‘3å¤©çš„æ•°æ®æ˜¯å¦å®Œæ•´
            if len(df) >= 3:
                recent_qv = df["quote_volume"].tail(3)
                if recent_qv.isna().any() or (recent_qv == 0).any():
                    print(f"[æ ¡éªŒ] {symbol}: æœ€è¿‘3å¤© quote_volume æ•°æ®ä¸å®Œæ•´")
                    invalid_symbols.append(symbol)

        except Exception as e:
            print(f"[æ ¡éªŒ] {symbol}: è¯»å–å¤±è´¥ - {e}")
            invalid_symbols.append(symbol)

    return invalid_symbols


def repair_invalid_caches(invalid_symbols: list):
    """
    ä¿®å¤æ— æ•ˆçš„ç¼“å­˜æ–‡ä»¶
    ç­–ç•¥: åˆ é™¤ç¼“å­˜æ–‡ä»¶ï¼Œè®© update_daily_cache é‡æ–°ä¸‹è½½å®Œæ•´æ•°æ®
    """
    cache_dir = Path(CONFIG["cache_dir"])

    for symbol in invalid_symbols:
        cache_path = cache_dir / f"{symbol}.csv"
        if cache_path.exists():
            print(f"[ä¿®å¤] åˆ é™¤æ— æ•ˆç¼“å­˜: {symbol}")
            cache_path.unlink()


# ============================================================
# ä¸»æµç¨‹
# ============================================================
def scan_pmarp_signals() -> tuple:
    """
    æ‰«ææ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„å¸ç§

    Returns:
        (signals, api_failed): ä¿¡å·åˆ—è¡¨å’ŒAPIæ˜¯å¦å¤±è´¥çš„æ ‡å¿—
    """
    print("=" * 50)
    print(f"[å¼€å§‹] PMARPä¿¡å·æ‰«æ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 50)

    # 0. é¢„æ£€æŸ¥ï¼šç¡®ä¿æ•°æ®æ˜¯æœ€æ–°çš„ä¸”æ ¼å¼æ­£ç¡®
    print("[æ­¥éª¤0] éªŒè¯æ•°æ®æ–°é²œåº¦...")
    if not ensure_data_updated():
        print("[é”™è¯¯] æ•°æ®æ›´æ–°å¤±è´¥ï¼Œæ— æ³•ç»§ç»­æ‰«æ")
        return [], True

    # 1. è·å–æˆäº¤é‡å‰Nçš„å¸ç§
    symbols = get_top_n_symbols(CONFIG["top_n"])

    if symbols is None:
        print("[é”™è¯¯] APIè¯·æ±‚å¤±è´¥ï¼Œæ— æ³•è·å–äº¤æ˜“å¯¹åˆ—è¡¨")
        return [], True  # APIå¤±è´¥

    if len(symbols) == 0:
        print("[é”™è¯¯] æœªè·å–åˆ°ä»»ä½•äº¤æ˜“å¯¹")
        return [], True  # APIå¼‚å¸¸

    # 1.5 æ ¡éªŒå¹¶ä¿®å¤ç¼“å­˜æ•°æ®å®Œæ•´æ€§
    print(f"[æ­¥éª¤1.5] æ ¡éªŒç¼“å­˜æ•°æ®å®Œæ•´æ€§...")
    invalid_symbols = validate_cache_integrity(symbols)
    if invalid_symbols:
        print(f"[æ ¡éªŒ] å‘ç° {len(invalid_symbols)} ä¸ªå¸ç§ç¼“å­˜æ•°æ®ä¸å®Œæ•´ï¼Œæ­£åœ¨ä¿®å¤...")
        repair_invalid_caches(invalid_symbols)

    # 2. é€ä¸ªæ›´æ–°ç¼“å­˜å¹¶æ£€æµ‹ä¿¡å·
    signals = []
    total = len(symbols)

    for idx, symbol in enumerate(symbols, 1):
        print(f"\n[{idx}/{total}] å¤„ç† {symbol}...")

        # æ£€æŸ¥æ˜¯å¦é¦–æ¬¡æ‰«æï¼ˆç¼“å­˜ä¸å­˜åœ¨ï¼‰
        is_first_scan = not get_cache_path(symbol).exists()

        # æ›´æ–°ç¼“å­˜
        df = update_daily_cache(symbol)

        if df.empty or len(df) < CONFIG["lookback"] + 10:
            print(f"  -> æ•°æ®ä¸è¶³ï¼Œè·³è¿‡")
            continue

        # è®¡ç®—PMARP
        pmarp = calculate_pmarp(
            df["close"],
            ema_period=CONFIG["ema_period"],
            lookback=CONFIG["lookback"]
        )

        valid_pmarp = pmarp.dropna()
        if len(valid_pmarp) == 0:
            continue

        curr_pmarp = valid_pmarp.iloc[-1]
        prev_pmarp = valid_pmarp.iloc[-2] if len(valid_pmarp) >= 2 else 0

        # æ£€æµ‹ä¿¡å·ï¼šä¸Šç©¿98% æˆ– é¦–æ¬¡æ‰«æä¸”å·²>=98%
        is_crossover = prev_pmarp < CONFIG["threshold"] and curr_pmarp >= CONFIG["threshold"]
        is_first_scan_high = is_first_scan and curr_pmarp >= CONFIG["threshold"]

        if is_crossover or is_first_scan_high:
            signal = {
                "symbol": symbol,
                "price": df["close"].iloc[-1],
                "pmarp": curr_pmarp,
                "prev_pmarp": prev_pmarp,
                "timestamp": df["timestamp"].iloc[-1].strftime("%Y-%m-%d"),
                "is_first_scan": is_first_scan_high and not is_crossover
            }
            signals.append(signal)
            if is_first_scan_high and not is_crossover:
                print(f"  -> æ–°å¸è§¦å‘! PMARP: {curr_pmarp:.1f}% (é¦–æ¬¡æ‰«æ)")
            else:
                print(f"  -> è§¦å‘ä¿¡å·! PMARP: {prev_pmarp:.1f}% -> {curr_pmarp:.1f}%")
        else:
            print(f"  -> PMARP: {curr_pmarp:.1f}%")

        # é™æµï¼šæ¯10ä¸ªè¯·æ±‚ä¼‘æ¯1ç§’
        if idx % 10 == 0:
            time.sleep(1)

    return signals, False  # æˆåŠŸå®Œæˆæ‰«æ


def main():
    """ä¸»å‡½æ•°"""
    try:
        # æ‰«æä¿¡å·
        signals, api_failed = scan_pmarp_signals()

        print("\n" + "=" * 50)
        print("[ç»“æœ]")
        print("=" * 50)

        if api_failed:
            # APIå¤±è´¥ï¼Œå‘é€é”™è¯¯é€šçŸ¥
            print("[é”™è¯¯] æ‰«æå¤±è´¥ï¼ŒAPIè¯·æ±‚é”™è¯¯")
            now = datetime.now().strftime("%Y-%m-%d %H:%M")
            message = f"*PMARP æ‰«æå¤±è´¥*\næ—¶é—´: {now}\n\nâš ï¸ APIè¯·æ±‚å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥\n\næ‰«æèŒƒå›´: 24hæˆäº¤é‡å‰{CONFIG['top_n']}"
            send_telegram_alert(message)
        elif signals:
            print(f"å‘ç° {len(signals)} ä¸ªä¿¡å·:")
            for s in signals:
                print(f"  - {s['symbol']}: ${s['price']:,.2f}, PMARP {s['pmarp']:.1f}%")

            # å‘é€Telegram
            message = format_signal_message(signals)
            send_telegram_alert(message)
        else:
            print("ä»Šæ—¥æ— ä¿¡å·")
            # å‘é€æ— ä¿¡å·é€šçŸ¥
            now = datetime.now().strftime("%Y-%m-%d %H:%M")
            message = f"*PMARP æ‰«æå®Œæˆ*\næ—¶é—´: {now}\n\nä»Šæ—¥æ— æ–°ä¿¡å·è§¦å‘\n\næ‰«æèŒƒå›´: 24hæˆäº¤é‡å‰{CONFIG['top_n']}"
            send_telegram_alert(message)

        print(f"\n[å®Œæˆ] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    except Exception as e:
        print(f"[è‡´å‘½é”™è¯¯] {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
